{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azraar/frist/blob/master/pytorch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMKWQZBq_wRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms,datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlnTuzvqP2cF",
        "colab_type": "text"
      },
      "source": [
        "Above, we're just loading in the dataset, shuffling it, and applying any transforms/pre-processing to it.\n",
        "\n",
        "Next, we need to handle for how we're going to iterate over that dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9VDnvJkAM5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "1e06e407-fdd9-401a-f279-6832dcf2e16d"
      },
      "source": [
        "\n",
        "train = datasets.MNIST('',train=True, download=True,\n",
        "                   transform=transforms.Compose([transforms.ToTensor()]))\n",
        "  \n",
        "test =  datasets.MNIST('',train=False, download=True,\n",
        "                   transform = transforms.Compose([transforms.ToTensor()]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 9321950.16it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 142425.26it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2328205.56it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 51783.87it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z6T5gKpCUwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset = torch.utils.data.DataLoader(train, batch_size=10,shuffle=True)\n",
        "testset = torch.utils.data.DataLoader(test, batch_size=10,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMXlMRsUP8gO",
        "colab_type": "text"
      },
      "source": [
        "You'll see later why this torchvision stuff is basically cheating! For now though, what have we done? Well, quite a bit.\n",
        "\n",
        "Training and Testing data split\n",
        "To train any machine learning model, we want to first off have training and validation datasets. This is so we can use data that the machine has never seen before to \"test\" the machine.\n",
        "\n",
        "Shuffling\n",
        "Then, within our training dataset, we generally want to randomly shuffle the input data as much as possible to hopefully not have any patterns in the data that might throw the machine off.\n",
        "\n",
        "For example, if you fed the machine a bunch of images of zeros, the machine would learn to classify everything as zero. Then you'd start feeding it ones, and the machine would figure out pretty quick to classify everything as ones...and so on. Whenever you stop, the machine would probably just classify everything as the last thing you trained on. If you shuffle the data, your machine is much more likely to figure out what's what.\n",
        "\n",
        "Scaling and normalization\n",
        "Another consideration at some point in the pipeline is usually scaling/normalization of the dataset. In general, we want all input data to be between zero and one. Often many datasets will contain data in ranges that are not within this range, and we generally will want to come up with a way to scale the data to be within this range.\n",
        "\n",
        "For example, an image is comprised of pixel values, most often in the range of 0 to 255. To scale image data, you usually just divide by 255. That's it. Even though all features are just pixels, and all you do is divide by 255 before passing to the neural network, this makes a huge difference.\n",
        "\n",
        "Batches\n",
        "Once you've done all this, you then want to pass your training dataset to your neural network.\n",
        "\n",
        "Not so fast!\n",
        "\n",
        "There are two major reasons why you can't just go and pass your entire dataset at once to your neural network:\n",
        "\n",
        "Neural networks shine and outperform other machine learning techniques because of how well they work on big datasets. Gigabytes. Terabytes. Petabytes! When we're just learning, we tend to play with datasets smaller than a gigabyte, and we can often just toss the entire thing into the VRAM of our GPU or even more likely into RAM.\n",
        "Unfortunately, in practice, you would likely not get away with this.\n",
        "\n",
        "The aim with neural networks is to have the network generalize with the data. We want the neural network to actually learn general principles. That said, neural networks often have millions, or tens of millions, of parameters that they can tweak to do this. This means neural networks can also just memorize things. While we hope neural networks will generalize, they often learn to just memorize the input data. Our job as the scientist is to make it as hard as possible for the neural network to just memorize.\n",
        "This is another reason why we often track \"in sample\" validation acccuracy and \"out of sample\" validation accuracy. If these two numbers are similar, this is good. As they start to diverge (in sample usually goes up considerably while out of sample stays the same or drops), this usually means your neural network is starting to just memorize things.\n",
        "\n",
        "One way we can help the neural network to not memorize is, at any given time, we feed only some specific batch size of data. This is often something between 8 and 64.\n",
        "\n",
        "Although there is no actual reason for it, it's a common trend in neural networks to use base 8 numbers for things like neuron counts, batch sizes...etc.\n",
        "\n",
        "This batching helps because, with each batch, the neural network does a back propagation for new, updated weights with hopes of decreasing that loss.\n",
        "\n",
        "With one giant passing of your data, this would include neuron changes that had nothing to do with general principles and were just brute forcing the operation.\n",
        "\n",
        "By passing many batches, each with their own gradient calcs, loss, and backprop, this means each time the neural network optimizes things, it will sort of \"keep\" the changes that were actually useful, and erode the ones that were just basically memorizing the input data.\n",
        "\n",
        "Given a large enough neural network, however, even with batches, your network can still just simply memorize.\n",
        "\n",
        "This is also why we often try to make the smallest neural network as possible, so long as it still appears to be learning. In general, this will be a more successful model long term.\n",
        "\n",
        "Now what?\n",
        "\n",
        "Well, we have our data, but what is it really? How do we work with it? We can iterate over our data like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaGlPJUMGRi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "06581cc6-3e93-4c26-d5e4-2f5c3fd79e6b"
      },
      "source": [
        "for data in trainset:\n",
        "  print(data)\n",
        "  break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([5, 3, 8, 3, 4, 1, 9, 7, 0, 1])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN6TQcyQQZ0a",
        "colab_type": "text"
      },
      "source": [
        "Each iteration will contain a batch of 10 elements (that was the batch size we chose), and 10 classes. Let's just look at one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk-3-FRnH1xz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0923a83e-135d-4330-f4c0-5d97361aebf5"
      },
      "source": [
        "x, y = data[0][0],data[1][0]\n",
        "print(y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsneCBGNQiXt",
        "colab_type": "text"
      },
      "source": [
        "X is our input data. The features. The thing we want to predict. y is our label. The classification. The thing we hope the neural network can learn to predict. We can see this by doing.\n",
        "\n",
        "data[0] is a bunch of features for things and data[1] is all the targets. So:\n",
        "\n",
        "tensor([5, 3, 8, 3, 4, 1, 9, 7, 0, 1])\n",
        "As you can see, data[1] is just a bunch of labels. So, since data[1][0] is a 5, we can expect data[0][0] to be an image of a 5. Let's see!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvNd-0cdJFAh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "cd2b7387-2ee7-4dd4-e742-9edb58abe153"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "plt.imshow(data[0][0].view(28,28))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEX4gKMIPBEX4gaD+Bxc8UDzMbH/DAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-Nf5hVTRN6u",
        "colab_type": "text"
      },
      "source": [
        "Clearly a 5 indeed.\n",
        "\n",
        "So, for our checklist:\n",
        "\n",
        "We've got our data of various featuresets and their respective classes.\n",
        "That data is all numerical.\n",
        "We've shuffled the data.\n",
        "We've split the data into training and testing groups.\n",
        "Is the data scaled?\n",
        "Is the data balanced?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91jUH7sDKcVK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc7c823f-4866-4f66-8ae9-e10b6e4b3aaa"
      },
      "source": [
        "print(data[0][0].shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS-usQKdRmEN",
        "colab_type": "text"
      },
      "source": [
        "What is data balancing?\n",
        "\n",
        "Recall before how I explained that if we don't shuffle our data, the machine will learn things like what the last few hundred classes were in a row, and probably just predict that from there on out.\n",
        "\n",
        "Well, with data balancing, a similar thing could occur.\n",
        "\n",
        "Imagine you have a dataset of cats and dogs. 7200 images are dogs, and 1800 are cats. This is quite the imbalance. The classifier is highly likely to find out that it can very quickly and easily get to a 72% accuracy by simple always predicting dog. It is highly unlikely that the model will recover from something like this.\n",
        "\n",
        "Other times, the imbalance isn't quite as severe, but still enough to make the model almost always predict a certain way except in the most obvious-to-it-of cases. Anyway, it's best if we can balance the dataset.\n",
        "\n",
        "By \"balance,\" I mean make sure there are the same number of examples for each classifications in training.\n",
        "\n",
        "Sometimes, this simply isn't possible. There are ways for us to handle for this with special class weighting for the optimizer to take note of, but, even this doesn't always work. Personally, I've never had success with this in any real world application.\n",
        "\n",
        "In our case, how might we confirm the balance of data? Well, we just need to iterate over everything and make a count. Pretty simple:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZKztshDOSIO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d96c90b9-85a9-40c3-c63e-edacaa0c4f4b"
      },
      "source": [
        "total = 0\n",
        "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
        "\n",
        "\n",
        "for data in trainset:\n",
        "    Xs, ys = data\n",
        "    for y in ys:\n",
        "        counter_dict[int(y)] += 1\n",
        "        total += 1\n",
        "\n",
        "print(counter_dict)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bknbBOTOPN0e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ec22c2b5-855e-4123-ebec-ec6fa7f1d19d"
      },
      "source": [
        "for i in counter_dict:\n",
        "    print(f\"{i}: {counter_dict[i]/total*100.0}%\") "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 9.871666666666666%\n",
            "1: 11.236666666666666%\n",
            "2: 9.93%\n",
            "3: 10.218333333333334%\n",
            "4: 9.736666666666666%\n",
            "5: 9.035%\n",
            "6: 9.863333333333333%\n",
            "7: 10.441666666666666%\n",
            "8: 9.751666666666667%\n",
            "9: 9.915000000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Vqum4CRwet",
        "colab_type": "text"
      },
      "source": [
        "I am sure there's a better way to do this, and there might be a built-in way to do it with torchvision. Anyway, as you can see, the lowest percentage is 9% and the highest is just over 11%. This should be just fine. We could balance this perfectly, but there's likely no need for that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wycZWvVPocM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}